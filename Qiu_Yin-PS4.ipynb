{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4, due March 16 at 11:59pm\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set to make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the TA sessions (especially the sections on vectorized computation and computational efficiency).\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 4.3 will be relatively painless or incredibly painful. \n",
    "* Part 4 (especially 4.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
    "* Do the extra credit problems last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using the [Boston Housing Prices Data Set](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt  \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load you data the Boston Housing data into a dataframe\n",
    "# MEDV.txt containt the median house values and data.txt the other 13 features\n",
    "# in order [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"]\n",
    "data = np.loadtxt('data.txt')\n",
    "target = np.loadtxt('MEDV.txt')\n",
    "bdata_df = pd.DataFrame(data, columns = ['CRIM','ZN','INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
    "bdata_df['MEDV'] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Getting oriented\n",
    "\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Using the same Boston housing prices dataset, use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn or the [OLS class](http://wiki.scipy.org/Cookbook/OLS) from SciPy to explore the relationship between  median housing price and number of rooms per house. Do this by first (a) regressing the housing price on the number of rooms per house, and then (b) regressing the housing price on the number of rooms per house and the (number of rooms per house) squared.  Interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept -35.576206874519855 and slopes [8.95992721] from regression(a)\n",
      "intercept 71.73632810734314 and slopes [-23.78960283   2.46914488] from regression(b)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model as lm\n",
    "# (a) OLS: 'RM' for 'MEDV'\n",
    "# use reshape(-1,1) for a single feature, reshape(-1,2) for two features\n",
    "\n",
    "X1 = np.array(bdata_df['RM']).reshape(-1,1)\n",
    "y = np.array(bdata_df['MEDV'])\n",
    "reg1 = lm.LinearRegression().fit(X1,y)\n",
    "print(\"intercept {} and slopes {} from regression(a)\".format(reg1.intercept_, reg1.coef_))\n",
    "\n",
    "# (b) OLS: 'RM' and 'RM'^2 for 'MEDV'\n",
    "X2 = np.array(pd.concat([bdata_df['RM'], bdata_df['RM'].pow(2)], axis=1))\n",
    "reg2 = lm.LinearRegression().fit(X2,y)\n",
    "print(\"intercept {} and slopes {} from regression(b)\".format(reg2.intercept_, reg2.coef_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.473\n",
      "Model:                            OLS   Adj. R-squared:                  0.472\n",
      "Method:                 Least Squares   F-statistic:                     452.3\n",
      "Date:                Mon, 16 Mar 2020   Prob (F-statistic):           4.12e-72\n",
      "Time:                        18:23:18   Log-Likelihood:                -1678.2\n",
      "No. Observations:                 506   AIC:                             3360.\n",
      "Df Residuals:                     504   BIC:                             3369.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -35.5762      2.748    -12.944      0.000     -40.976     -30.176\n",
      "x1             8.9599      0.421     21.267      0.000       8.132       9.788\n",
      "==============================================================================\n",
      "Omnibus:                      105.253   Durbin-Watson:                   0.705\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              604.717\n",
      "Skew:                           0.763   Prob(JB):                    4.87e-132\n",
      "Kurtosis:                       8.133   Cond. No.                         61.7\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.539\n",
      "Model:                            OLS   Adj. R-squared:                  0.538\n",
      "Method:                 Least Squares   F-statistic:                     294.5\n",
      "Date:                Mon, 16 Mar 2020   Prob (F-statistic):           2.19e-85\n",
      "Time:                        18:23:18   Log-Likelihood:                -1644.1\n",
      "No. Observations:                 506   AIC:                             3294.\n",
      "Df Residuals:                     503   BIC:                             3307.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         71.7363     12.865      5.576      0.000      46.460      97.013\n",
      "x1           -23.7896      3.867     -6.152      0.000     -31.387     -16.192\n",
      "x2             2.4691      0.290      8.513      0.000       1.899       3.039\n",
      "==============================================================================\n",
      "Omnibus:                       83.768   Durbin-Watson:                   0.715\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              926.703\n",
      "Skew:                           0.270   Prob(JB):                    5.87e-202\n",
      "Kurtosis:                       9.608   Cond. No.                     2.13e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.13e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# alternatively, using statsmodels to get summary table\n",
    "import statsmodels.api as sm\n",
    "X1a = sm.add_constant(X1)\n",
    "model1 = sm.OLS(y, X1a).fit()\n",
    "print(model1.summary())\n",
    "\n",
    "X2a = sm.add_constant(X2)\n",
    "model2 = sm.OLS(y, X2a).fit()\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first regression, on average 1 additional room per house is associated with 8960 USD increase in median value of owner-occupied homes.\n",
    "\n",
    "In the second regression, the polynomial regression is non-linear. The change of median value of homes is depending on the number or rooms per house. The slope is not constant. For example, when room number increases from 6 to 7, the median housing value is estimated to increase by 8308 USD; when room number increases from 8 to 9, the median housing value is estimated to increase by 18184 USD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training and testing\n",
    "\n",
    "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use 25-fold cross-validation to fit regression (a) above, i.e. the linear fit of housing price on number of rooms per house. Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Plot the distribution of the 25 slope coefficients using a histogram, then draw a vertical line at the value of the slope coefficient that you estimated in 1.1 using the full dataset.  What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=25)\n",
    "\n",
    "coef_arr = np.array([])\n",
    "for train_index, test_index in kf.split(X1):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X1[train_index], X1[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    reg = lm.LinearRegression().fit(X_train, y_train)\n",
    "    coef_arr = np.append(coef_arr, reg.coef_)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(coef_arr)\n",
    "plt.axvline(reg1.coef_, color='r', linestyle='--')\n",
    "plt.title('Distribution of 25 slope coefficients using k-fold cross-validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope coefficient from 1.1 is at about center of the distribution, and the variation of 25 folds is high, ranging from about 8.5 to 9.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Regression lines\n",
    "\n",
    "Create a scatterplot of housing price on rooms per house, and add the two regression lines from 1.1 (or 1.2 if you prefer to do so). Show the linear regression line in red, and the linear+quadratic regression line (which may have curvature) in blue.  Label these two regression lines with the corresponding regression equations (i.e. the slope and intercept of the line).\n",
    "\n",
    "Bonus: Add the 95% confidence bands (i.e.,  the area that has a 95% chance of containing the true regression line) to each of these lines.\n",
    "\n",
    "Note: You can get 2 points even without the confidence bands (if the other lines are correct, the axes are labeled, the lines are labeled, the points are plotted correctly, and so forth). If you do everything perfectly including the confidence bands, you get 2 points. If you do something else wrong and would normally receive less than two points, the confidence intervals can replace lost points up to 0.5.\n",
    "\n",
    "Useful reference: [1](https://www.medcalc.org/manual/scatter_diagram_regression_line.php), [2](https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals)\n",
    "\n",
    "You can directly use some packages to calculate the bands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# scatter plot of RM and MDEV\n",
    "plt.scatter(X1, y, c='g', \n",
    "            marker='.', label='scatter plot of RM vs MDEV')\n",
    "# linear regression line from 1.1 (a)\n",
    "plt.scatter(X1, np.dot(X1, reg1.coef_) + reg1.intercept_, \n",
    "         c='r', label='y = 8.96 * x - 35.58')\n",
    "# non-linear regression line from 1.1 (b)\n",
    "plt.scatter(X1, np.dot(X2, reg2.coef_) + reg2.intercept_, \n",
    "         c='b', label='y = -23.79 * x + 2.47 * x^2 + 71.74')\n",
    "\n",
    "plt.title('Regression lines and scatter plot (RM vs MEDV)')\n",
    "plt.xlabel('RM')\n",
    "plt.ylabel('Median Home Values')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient descent: Linear Regression\n",
    "\n",
    "This is where it gets fun!\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (average rooms per house)\n",
    "\n",
    "Implement the batch gradient descent algorithm that we discussed in class. Use the version you implement to regress the housing price on the number of rooms per house. Experiment with 3-4 different values of the learning rate *R*, and do the following:\n",
    "\n",
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of *R*)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "* How do your coefficients compare to the ones estimated through standard libraries? Does this depend on *R*?\n",
    "\n",
    "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
    "\n",
    "* *Hint 1: Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to (a) check to see if the loss has stopped decreasing; and (b) check if both your current parameter esimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 0.0001).*\n",
    "* *Hint 2: Some people like to include a MaxIterations parameter in their gradient descent algorithm, to prevent divergence. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "bivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: float\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "def bivariate_ols(xvalues, yvalues, R=0.01, MaxIterations=1000):\n",
    "    start_time = time.time()\n",
    "    # initialize the parameters\n",
    "    alpha_old = np.random.rand()\n",
    "    beta_old = np.random.rand()\n",
    "    alpha = np.random.rand()\n",
    "    beta = np.random.rand()\n",
    "    n = len(xvalues)\n",
    "    \n",
    "    # tolerence and iteration counter\n",
    "    epsilon = 1e-4\n",
    "    cnt = 0\n",
    "    cost = 0\n",
    "    while (np.abs(alpha_old - alpha) > epsilon or np.abs(beta_old - beta) > epsilon):\n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "        alpha = alpha_old - R / n * ((alpha_old + beta_old * xvalues - yvalues).sum())\n",
    "        beta = beta_old - R / n * (((alpha_old + beta_old * xvalues - yvalues) * xvalues).sum())\n",
    "        # print the cost to check it is decreasing\n",
    "        cost = 1.0 / (2*n) * (((yvalues - alpha - beta * xvalues) ** 2).sum())\n",
    "#         print(cost)\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt > MaxIterations:\n",
    "            print('Iterations exceeded maximum number. Stopping...')\n",
    "            break\n",
    "\n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return alpha, beta, cnt, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.29 seconds\n",
      "Learning Rate=0.045\n",
      " (-35.381775132200936, 8.930289855150649, 10165, 22.246158244315833) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function call\n",
    "print('Learning Rate=0.045\\n', bivariate_ols(np.array(bdata_df['RM']), y, 0.045, 100000), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.32 seconds\n",
      "Learning Rate=0.04\n",
      " (-35.35748727276446, 8.926587640921143, 11213, 22.246216878773375) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate=0.04\\n', bivariate_ols(np.array(bdata_df['RM']), y, 0.04, 100000), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.40 seconds\n",
      "Learning Rate=0.03\n",
      " (-35.284544601339725, 8.915468942017558, 14102, 22.24643441490863) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate=0.03\\n', bivariate_ols(np.array(bdata_df['RM']), y, 0.03, 100000), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When R = 0.045, we get alpha = 8.93, beta = -35.38 that minimizes the loss function.\n",
    "It takes 10201 iterations to converge. The total runtime is 0.30 second.\n",
    "\n",
    "When R = 0.04, we get alpha = 8.93, beta = -35.36 that minimizes the loss function. It takes 11296 iterations to converge. The total runtime is 0.33 second.\n",
    "\n",
    "When R = 0.03, we get alpha = 8.92, beta = -35.28 that minimizes the loss function.\n",
    "It takes 14062 iterations to converge. The total runtime is 0.40 second.\n",
    "\n",
    "All of the three learning rates get us very close estimations to what we got in 1.1 (alpha=8.96, beta=-35.58). Sufficiently small R can converge faster, and these R are good enough to estimate alpha and beta for the accuracy I specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data normalization (done for you!)\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, we want to give you some code in case you want to standardize your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(raw_data):\n",
    "    return ((raw_data - np.mean(raw_data, axis = 0)) / np.std(raw_data, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using CRIM and RM as independent variables. Standardize these variables before before inputting them to the gradient descent algorithm. \n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with 2-3 different values of R.\n",
    "\n",
    "* *Hint 1: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalue_matrix, yvalues : narray\n",
    "    xvalue_matrix: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "def multivariate_ols(xvalues, yvalues, R=0.01, MaxIterations=1000):\n",
    "    start_time = time.time()\n",
    "    # Initialize variablse\n",
    "    N, m = np.shape(xvalues)\n",
    "    alpha_old = np.random.rand()\n",
    "    beta_old = np.random.rand(m)\n",
    "    alpha = np.random.rand()\n",
    "    beta = np.random.rand(m)\n",
    "    \n",
    "    # tolerance and iteration counter\n",
    "    cost_list = np.empty(1)\n",
    "    epsilon = 1e-4\n",
    "    cnt = 0\n",
    "    while(np.abs(alpha_old-alpha) > epsilon or np.max(np.abs(beta_old-beta)) > epsilon):\n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "        alpha = alpha_old - R / N * ((alpha_old +  np.dot(xvalues, beta_old) - yvalues).sum())\n",
    "        beta = beta_old - R / N * np.dot(xvalues.T, (alpha_old +  np.dot(xvalues, beta_old) - yvalues))\n",
    "        \n",
    "        # print the cost to check it is descreasing\n",
    "        cost = 1.0 / (2* N) * np.linalg.norm(yvalues - alpha - np.dot(xvalues, beta))**2\n",
    "        cost_list= np.append(cost_list, cost)\n",
    "#         print(cost_list)\n",
    "        \n",
    "        # If it takes too many iterations terminate\n",
    "        cnt += 1\n",
    "        if cnt > MaxIterations:\n",
    "            print('Iterations exceeded maximum number. Stopping...')\n",
    "            break\n",
    "        \n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))    \n",
    "    return alpha, beta, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.01 seconds\n",
      "Learning Rate=0.05\n",
      " (22.530956538655232, array([-2.24361178,  5.8169275 ]), 183) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function call\n",
    "print('Learning Rate=0.05\\n', multivariate_ols(standardize(np.array(bdata_df[['CRIM','RM']])), y, 0.05, 100000), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.01 seconds\n",
      "Learning Rate=0.04\n",
      " (22.530462284606713, array([-2.24360706,  5.81690701]), 224) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate=0.04\\n', multivariate_ols(standardize(np.array(bdata_df[['CRIM','RM']])), y, 0.04, 100000), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.03 seconds\n",
      "Learning Rate=0.02\n",
      " (22.527993572980545, array([-2.2443569 ,  5.81601407]), 418) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate=0.02\\n', multivariate_ols(standardize(np.array(bdata_df[['CRIM','RM']])), y, 0.02, 100000), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When R = 0.05, we get alpha = 22.53, beta = (-2.24, 5.82) that minimizes the loss function.\n",
    "It takes 183 iterations to converge. The total runtime is 0.01 second.\n",
    "\n",
    "When R = 0.04, we get alpha = 22.53, beta = (-2.24, 5.82) that minimizes the loss function. It takes 224 iterations to converge. The total runtime is 0.02 second.\n",
    "\n",
    "When R = 0.02, we get alpha = 22.53, beta = (-2.24, 5.82) that minimizes the loss function.\n",
    "It takes 418 iterations to converge. The total runtime is 0.03 second.\n",
    "\n",
    "Sufficiently small learning rates can provide good enough estimations of coefficient and intercept. Just that larger ones tend to be faster and have less iterations and less runtime.\n",
    "\n",
    "Interpretation: holding other factors constant, 1 more room per house indicates an increase of 5820 USD in median housing value on average; holding others constant, 1 more percentage crime rate per capita indicates a decrease of 2244 USD in median housing value on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.01 seconds\n",
      "Learning Rate=0.1\n",
      " (22.531927490939758, array([-2.24344151,  5.8171437 ]), 96) \n",
      "\n",
      "Time taken: 0.03 seconds\n",
      "Learning Rate=0.01\n",
      " (22.522950009832563, array([-2.24576922,  5.81424156]), 766) \n",
      "\n",
      "Time taken: 0.23 seconds\n",
      "Learning Rate=0.001\n",
      " (22.432969090545296, array([-2.25124496,  5.7985267 ]), 5412) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardized\n",
    "print('Learning Rate=0.1\\n', multivariate_ols(standardize(np.array(bdata_df[['CRIM','RM']])), y, 0.1, 100000), '\\n')\n",
    "print('Learning Rate=0.01\\n', multivariate_ols(standardize(np.array(bdata_df[['CRIM','RM']])), y, 0.01, 100000), '\\n')\n",
    "print('Learning Rate=0.001\\n', multivariate_ols(standardize(np.array(bdata_df[['CRIM','RM']])), y, 0.001, 100000), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.03 seconds\n",
      "Learning Rate=0.1\n",
      " (nan, array([nan, nan]), 325) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qyinhelena/anaconda3/envs/APIs_geospatial/lib/python3.7/site-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.48 seconds\n",
      "Learning Rate=0.01\n",
      " (-29.035658364758554, array([-0.26419927,  8.10758375]), 32902) \n",
      "\n",
      "Iterations exceeded maximum number. Stopping...\n",
      "Time taken: 5.45 seconds\n",
      "Learning Rate=0.001\n",
      " (-19.210713339990537, array([-0.29670662,  6.62776151]), 100001) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Non-standardized\n",
    "print('Learning Rate=0.1\\n', multivariate_ols(np.array(bdata_df[['CRIM','RM']]), y, 0.1, 100000), '\\n')\n",
    "print('Learning Rate=0.01\\n', multivariate_ols(np.array(bdata_df[['CRIM','RM']]), y, 0.01, 100000), '\\n')\n",
    "print('Learning Rate=0.001\\n', multivariate_ols(np.array(bdata_df[['CRIM','RM']]), y, 0.001, 100000), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When non-standardized, the loss function becomes more \"spherical\" and it takes so much longer runtime to converge, even for the same R. When R=0.1 and X-array non-standardized, it diverges. When R=0.001 and X-array is non-standardized, it exceeds the maximum iteration limit, but still not reaching the local minima. When R=0.01, it converges but it takes so much longer time and more iterations. Thus, standardization is necessary to efficiently and accurately find local minima in GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Implement  Mini-Batch Gradient Descent (MB-GD)**\n",
    "MB-GD is a Gradient Descent variant that in large data sets can converge faster and is computationally less intensive. Implement MB-GD for question 2.3. Tune the learning rate, number of iterations and \"mini-batch\" size\n",
    "so that you compute the estimates within a 1e-2 tolerance. Do not use a batch-size greater than 32.\n",
    "In summary you go over the entire data set for n epochs. At the beginning of each epoch you shuffle your data once and then you pick k batches (approximately k=#of data points/batch_size). For each batch you compute the gradient, update the parameters and move to the next batch.\n",
    "MB-GD is similar to Stochastic Gradient Descent but instead of using one sample to compute the gradient we use a batch of samples at each iteration. You can find details about MB-GD here:\n",
    "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mb_gd(xvalues, yvalues, R=0.01, batch_size=32, MaxIter=10000):\n",
    "#     start_time = time.time()\n",
    "#     # Initialize variablse\n",
    "#     N, m = np.shape(xvalues)\n",
    "#     alpha_old = np.random.rand()\n",
    "#     beta_old = np.random.rand(m)\n",
    "#     alpha = np.random.rand()\n",
    "#     beta = np.random.rand(m)\n",
    "#     cost_old = np.inf\n",
    "#     cost_new = np.linalg.norm(yvalues)\n",
    "#     xvalues_init = xvalues\n",
    "#     yvalues_init = yvalues\n",
    "    \n",
    "#     # tolerance, iteration counter and n epoches\n",
    "#     cost_list = np.empty(1)\n",
    "#     epsilon = 1e-6\n",
    "#     cnt = 0\n",
    "#     n_epoch = int(N/batch_size)\n",
    "    \n",
    "#     for epoch in range(n_epoch+1): # random shuffle for each epoch\n",
    "#         ind = np.random.choice(N, batch_size, replace=False)\n",
    "#         xvalues = xvalues_init[ind, :]\n",
    "#         yvalues = yvalues_init[ind]\n",
    "#         N, _ = xvalues.shape\n",
    "\n",
    "#         # update alpha beta for each batch\n",
    "#         for data in range(batch_size):\n",
    "#             cnt += 1\n",
    "#             if (np.abs(alpha_old-alpha) > epsilon or np.max(np.abs(beta_old-beta)) > epsilon):\n",
    "#                 alpha_old = alpha\n",
    "#                 beta_old = beta\n",
    "\n",
    "#                 alpha = alpha_old - R / N * ((alpha_old +  np.dot(xvalues, beta_old) - yvalues).sum())\n",
    "#                 beta = beta_old - R / N * np.dot(xvalues.T, (alpha_old +  np.dot(xvalues, beta_old) - yvalues))\n",
    "\n",
    "#                 cost = 1.0 / (2* N) * np.linalg.norm(yvalues - alpha - np.dot(xvalues, beta))**2\n",
    "#                 cost_list= np.append(cost_list, cost)\n",
    "# #                 print(cost_list)\n",
    "                \n",
    "#              #elif cnt > MaxIter:\n",
    "# #                 print('Iterations exceeded maximum number. Stopping...')\n",
    "# #                 break\n",
    "                \n",
    "#             else:\n",
    "#                 print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))   \n",
    "#                 return alpha, beta, cnt\n",
    "\n",
    "#     print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))    \n",
    "#     return alpha, beta, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "mb_gd\n",
    "    Gradient Decent with mini batches. Used to find co-efficients of regression that is convex\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalue, yvalues : narray\n",
    "    xvalue: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "BatchSize: Int\n",
    "    number of datapoints in a batch\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: array[float]\n",
    "    co-efficient\n",
    "    \n",
    "cnt: int\n",
    "    count of iterations\n",
    "\"\"\"\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def mb_gd(xvalues, yvalues, R=0.01, MaxIterations=1000, BatchSize=32):\n",
    "    start_time = time.time()\n",
    "    # Initiate variables\n",
    "    N, m = np.shape(xvalues)\n",
    "    alpha = np.random.rand()\n",
    "    beta = np.random.rand(m)\n",
    "    \n",
    "    # in this implementation I keep track of the objective function to determine convergence\n",
    "    cost_old = np.inf\n",
    "    cost_new = np.linalg.norm(yvalues)\n",
    "    epsilon = 1e-4\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(MaxIterations):\n",
    "        cost_old = cost_new\n",
    "        # shuffle data\n",
    "        Xb, yb = shuffle(xvalues, yvalues)\n",
    "        \n",
    "        # for each of the floor(n/K) batches do gradient steps\n",
    "        for offset in range(0, N, BatchSize):\n",
    "            \n",
    "            end = offset + BatchSize\n",
    "            batchx, batchy = Xb[offset:end], yb[offset:end]\n",
    "            \n",
    "            # update alpha and beta for each batch\n",
    "            alpha = alpha - R / N * ((alpha + np.dot(batchx, beta) - batchy).sum())\n",
    "            beta = beta - R / N * np.dot(batchx.T, (alpha +  np.dot(batchx, beta) - batchy))\n",
    "            # compute new cost\n",
    "            cost_new = 1.0 / (2 * N) * np.linalg.norm(batchy - alpha - np.dot(batchx, beta))**2\n",
    "            \n",
    "        if((np.abs(cost_old-cost_new) < epsilon) or (cnt > MaxIterations)):\n",
    "            print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))    \n",
    "            return alpha, beta, cnt\n",
    "            \n",
    "        cnt += 1\n",
    "        \n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))    \n",
    "    return alpha, beta, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept 22.53280632411067 and slopes [-2.24281888  5.8177965 ] from regression(a)\n"
     ]
    }
   ],
   "source": [
    "# OLS: regressing 'MEDV' on standardized CRIM and 'RM'\n",
    "# use reshape(-1,1) for a single feature, reshape(-1,2) for two features\n",
    "\n",
    "X3 = standardize(np.array(bdata_df[['CRIM','RM']]))\n",
    "y = np.array(bdata_df['MEDV'])\n",
    "reg3 = lm.LinearRegression().fit(X3,y)\n",
    "print(\"intercept {} and slopes {} from regression(a)\".format(reg3.intercept_, reg3.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.48 seconds\n",
      "Learning Rate=0.01 and 32 batches\n",
      " (22.53190997411802, array([-2.24306554,  5.81724333]), 1000) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate=0.01 and 32 batches\\n', mb_gd(standardize(np.array(bdata_df[['CRIM','RM']])), y, \n",
    "                                                   0.01, 1000, 32), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.47 seconds\n",
      "Learning Rate=0.1 and 32 batches\n",
      " (22.532249025191813, array([-2.2440273 ,  5.81877786]), 1000) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate=0.1 and 32 batches\\n', mb_gd(standardize(np.array(bdata_df[['CRIM','RM']])), y, \n",
    "                                                    0.1, 1000, 32), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.46 seconds\n",
      "Learning Rate=0.02 and 32 batches\n",
      " (22.533035925177334, array([-2.24222604,  5.81790327]), 1000) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate=0.02 and 32 batches\\n', mb_gd(standardize(np.array(bdata_df[['CRIM','RM']])), y, \n",
    "                                                    0.02, 1000, 32), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't observe speedup as a total. Although it iterate many times, the speed iterating for each batch is extremely fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction\n",
    "\n",
    "Let's use our fitted model to make predictions about housing prices. Make sure to first standardize your features before proceeding.\n",
    "\n",
    "### 3.1 Cross-Validation\n",
    "\n",
    "Unless you were careful above, you probably overfit your data again. Let's fix that in one of two ways. If you're feeling confident, use k-fold cross-validation to re-fit the multivariate regression from 2.3 above, and report your estimated coefficients (there should be three, corresponding to the intercept and the two coefficients for CRIM and RM). Or if you want to do the quick and dirty version, randomly divide your data into a training set (66%) and testing set (34%) and use the training set to re-fit the regression from 2.3 above. \n",
    "\n",
    "*NOTE: * If using k-fold cross-validation, you will end up with a different set of parameters for each fold. In this case, use the parameters from the fold that has the highest test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "compute_rmse\n",
    "\n",
    "Given two arrays, one of actual values and one of predicted values,\n",
    "compute the Roote Mean Squared Error\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "predictions : array\n",
    "    Array of numerical values corresponding to predictions for each of the N observations\n",
    "\n",
    "yvalues : array\n",
    "    Array of numerical values corresponding to the actual values for each of the N observations\n",
    "\n",
    "Returns\n",
    "-------\n",
    "rmse : int\n",
    "    Root Mean Squared Error of the prediction\n",
    "\"\"\"\n",
    "# compute RMSE\n",
    "def compute_rmse(predictions, yvalues):\n",
    "    pre = np.asarray(predictions)\n",
    "    y = np.asarray(yvalues)\n",
    "    rmse = np.sqrt(np.sum((pre-y) ** 2) / float(len(y)))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.46 seconds\n",
      "22.52532771721301 [-2.24384916  5.79313478] 6.382219873433155\n",
      "Time taken: 0.42 seconds\n",
      "22.682225781358785 [-2.30025563  5.7451674 ] 6.342751270730907\n",
      "Time taken: 0.54 seconds\n",
      "22.528203238057095 [-2.24440273  5.79881166] 6.412008882154863\n",
      "Time taken: 0.30 seconds\n",
      "22.54269945678394 [-2.24452616  5.82631673] 6.424991116401622\n",
      "Time taken: 0.26 seconds\n",
      "22.550467829471557 [-2.25308862  5.80135356] 6.4129561775759365\n",
      "Time taken: 0.46 seconds\n",
      "22.59815873047791 [-2.26847225  5.81655083] 6.413371567032139\n",
      "Time taken: 0.45 seconds\n",
      "22.698834006317224 [-2.30601617  5.77197478] 6.351780235456768\n",
      "Time taken: 0.45 seconds\n",
      "22.406434539383756 [-2.24518348  5.66287471] 6.274492021143502\n",
      "Time taken: 0.45 seconds\n",
      "22.35141338847525 [-2.18903918  5.74895141] 6.303104910517313\n",
      "Time taken: 0.45 seconds\n",
      "22.30077936132886 [-2.22863017  5.44557826] 6.267903211674273\n",
      "Time taken: 0.41 seconds\n",
      "22.425884190842286 [-2.20522477  5.8054697 ] 6.384079508460449\n",
      "Time taken: 0.38 seconds\n",
      "22.49800850395268 [-2.25167038  5.72014414] 6.375557428074489\n",
      "Time taken: 0.46 seconds\n",
      "22.39428623598153 [-2.24283861  5.59034284] 6.344115020625005\n",
      "Time taken: 0.43 seconds\n",
      "22.33325496680895 [-2.23305167  5.50468578] 6.290165192456881\n",
      "Time taken: 0.11 seconds\n",
      "22.297438916093558 [-2.25519857  5.72353478] 6.408862003795987\n",
      "Time taken: 0.45 seconds\n",
      "22.56326420647162 [-2.25263727  5.83549931] 6.417995984713761\n",
      "Time taken: 0.45 seconds\n",
      "22.59353908088757 [-2.2692264   5.81048713] 6.40282885049047\n",
      "Time taken: 0.04 seconds\n",
      "18.75724246774458 [-2.08374156  5.39220642] 7.295973707958621\n",
      "Time taken: 0.38 seconds\n",
      "22.192763526818652 [-2.38071619  6.33431942] 5.342063589614449\n",
      "Time taken: 0.14 seconds\n",
      "22.64981942790217 [-2.2058186   5.82127431] 6.285743320707669\n",
      "Time taken: 0.45 seconds\n",
      "22.4953652850577 [-2.69299752  6.03854598] 6.1337867807843125\n",
      "Time taken: 0.17 seconds\n",
      "22.859764743625423 [-1.95050287  5.91225377] 6.17130485385236\n",
      "Time taken: 0.52 seconds\n",
      "22.796762750403847 [-2.11698302  5.92123263] 6.2537618071726975\n",
      "Time taken: 0.36 seconds\n",
      "22.555106899812102 [-2.25118047  5.83759838] 6.395268477515647\n",
      "Time taken: 0.45 seconds\n",
      "22.618592345403467 [-2.27908219  5.80378428] 6.352641960038842\n"
     ]
    }
   ],
   "source": [
    "# slicing data by column name\n",
    "X_cr_rm = standardize(np.array(bdata_df[['CRIM','RM']]))\n",
    "\n",
    "# 25-fold cross validation on training data\n",
    "kf = KFold(n_splits=25)\n",
    "\n",
    "# iterate through 25 folds\n",
    "for train_index, test_index in kf.split(X_cr_rm):\n",
    "    X_train_cr_rm, X_test_cr_rm = X_cr_rm[train_index], X_cr_rm[test_index]\n",
    "    y_train_cr_rm, y_test_cr_rm = y[train_index], y[test_index]\n",
    "    \n",
    "    # fit the mini-batch gd model\n",
    "    inter4, coef4, iter4 = mb_gd(X_train_cr_rm, y_train_cr_rm, 0.02, 1000, 32)\n",
    "    \n",
    "    # predict y for current fold\n",
    "    pred_y = np.dot(X_train_cr_rm, coef4) + inter4\n",
    "    \n",
    "    # print intercept, coefficient and RMSE for each fold\n",
    "    print(inter4, coef4, compute_rmse(pred_y, y_train_cr_rm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When RMSE = 5.34, we have the highest performance from this fold. The intercept is 22.193, and CRIM and RM coefficients are -2.381, 6.334."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predicted values and RMSE\n",
    "\n",
    "Let's figure out how accurate this predictive model turned out to be. Compute the RMSE on your test cases, i.e. take the model parameters that you found above, predict the values for just the test instances, and compare the actual to the predicted values. If you did this the k-fold way above, this will be the average RMSE across the k test sets. If you did this the quick and dirty way above, this will just be the RMSE on your single test set.\n",
    "\n",
    "What is your test RMSE?  How does it compare to the performance of your nearest neighbor algorithm from the last problem set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE for testing set:  5.414911982915681\n"
     ]
    }
   ],
   "source": [
    "# 25-fold cross validation on testing data\n",
    "kf = KFold(n_splits=25)\n",
    "test_rmse = np.array([])\n",
    "for train_index, test_index in kf.split(X_cr_rm):\n",
    "    X_train_cr_rm, X_test_cr_rm = X_cr_rm[train_index], X_cr_rm[test_index]\n",
    "    y_train_cr_rm, y_test_cr_rm = y[train_index], y[test_index]\n",
    "    \n",
    "    # plug in parameters of lowest RMSE from 3.1 to predict y test\n",
    "    pred_y_t = np.dot(X_test_cr_rm, np.array([-2.381, 6.334])) + 22.193\n",
    "    test_rmse = np.append(test_rmse, compute_rmse(pred_y_t, y_test_cr_rm))\n",
    "\n",
    "print('Average RMSE for testing set: ', np.mean(test_rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average RMSE on test dataset is 5.41. The MB-GD algorithm performs better than NN algorithm in PS3, which was 7.37."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit 1: Logistic Regression\n",
    "\n",
    "For extra credit, implement logistic regression using gradient descent. Create a new variable (EXPENSIVE) to indicate whether the median housing price is more than $40,000. Use your model  a logistic regression of EXPENSIVE on CHAS and RM. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new binary column using lambda functions (dataframes)\n",
    "bdata_df['EXPENSIVE'] = bdata_df.apply(lambda x: 1 if x['MEDV'] > 40 else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_log_reg(xvalues, yvalues, R=0.01, MaxIterations=1000):\n",
    "    start_time = time.time()\n",
    "    # Initialize variablse\n",
    "    N, m = np.shape(xvalues)\n",
    "    alpha_old = np.random.rand()\n",
    "    beta_old = np.random.rand(m)\n",
    "    alpha = np.random.rand()\n",
    "    beta = np.random.rand(m)\n",
    "    \n",
    "    # tolerance and iteration counter\n",
    "    cost_list = np.empty(1)\n",
    "    epsilon = 1e-4\n",
    "    cnt = 0\n",
    "    while(np.abs(alpha_old-alpha) > epsilon or np.max(np.abs(beta_old-beta)) > epsilon):\n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "        vec_1 = 1.0 /(1 + np.exp(-(alpha_old + np.dot(xvalues, beta_old)))) - yvalues\n",
    "        alpha = alpha_old - R / N * (vec_1.sum())\n",
    "        beta = beta_old - R / N * (np.dot(np.transpose(xvalues), vec_1))\n",
    "        \n",
    "        # print the cost to check it is descreasing NEED TO CHANGE BELOW!!!\n",
    "#         cost = 1.0 / (2* N) * np.linalg.norm(yvalues - alpha - np.dot(xvalues, beta))**2\n",
    "#         cost_list= np.append(cost_list, cost)\n",
    "#         print(cost_list)\n",
    "        \n",
    "        # If it takes too many iterations terminate\n",
    "        cnt += 1\n",
    "        if cnt > MaxIterations:\n",
    "            print('Iterations exceeded maximum number. Stopping...')\n",
    "            break\n",
    "        \n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))    \n",
    "    return alpha, beta, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.20 seconds\n",
      "Learning Rate=0.1\n",
      " (-20.237799081999768, array([1.18285756, 2.48705379]), 50577)\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate=0.1\\n', multivariate_log_reg(np.array(bdata_df[['CHAS','RM']]),\n",
    "                                                   np.array(bdata_df['EXPENSIVE']),\n",
    "                                                   0.1, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.90 seconds\n",
      "Learning Rate=0.2\n",
      " (-21.920810672507358, array([1.2167098 , 2.71362919]), 37119)\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate=0.2\\n', multivariate_log_reg(np.array(bdata_df[['CHAS','RM']]),\n",
    "                                                   np.array(bdata_df['EXPENSIVE']),\n",
    "                                                   0.2, 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cofficients converge as shown above. If CHAS is 1, the probability is 90.25% for the median housing value to be EXPENSIVE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Regularization \n",
    "\n",
    "### 4.1 Get prepped\n",
    "\n",
    "Step 1: Create new interaction variables between each possible pair of the F_s features. If you originally had *K* features, you should now have K+(K*(K+1))/2 features. Standardize all of your features.\n",
    "\n",
    "Step 2: For simplicity, generate a single training and testing set.  Randomly sample 66% of your data and call this the training set, and set aside the remaining 34% as your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105) 104.0\n"
     ]
    }
   ],
   "source": [
    "# reload datasets to dataframe\n",
    "data = np.loadtxt('data.txt')\n",
    "target = np.loadtxt('MEDV.txt')\n",
    "bdata_df_new = pd.DataFrame(data, columns = ['CRIM','ZN','INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
    "\n",
    "# add new interaction variables between each possible pair\n",
    "k = 13\n",
    "for i in range(0, k):\n",
    "    for j in range(i, k):\n",
    "        bdata_df_new[str(bdata_df_new.columns[i] + '*' + bdata_df_new.columns[j])] = bdata_df_new.iloc[:,i]*bdata_df_new.iloc[:,j]\n",
    "\n",
    "bdata_df_new['MEDV'] = target # add target at the last column\n",
    "\n",
    "# check column number is correct, there is one more column for target and others are features\n",
    "print(bdata_df_new.shape, k+(k*(k+1))/2)\n",
    "# check and store column names\n",
    "column_names = list(bdata_df_new.columns)\n",
    "# print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the new dataframe with interaction terms into training and testing set by 66% and 34%\n",
    "bdata_X_train, bdata_X_test, bdata_y_train, bdata_y_test = train_test_split(np.array(bdata_df_new.iloc[:,0:105]), \n",
    "                                                                            np.array(bdata_df_new.iloc[:,-1]),\n",
    "                                                                            test_size=0.34, \n",
    "                                                                            random_state=13579, \n",
    "                                                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Overfitting (sort of)\n",
    "Now, using your version of multivariate regression from 2.3, let's overfit the training data. Using your training set, regress housing price on as many of those K+(K*(K+1))/2 features as you can (Don't forget to add quadratic terms. Form instance, RM^2.).  If you get too greedy, it's possible this will take a long time to compute, so start with 5-10 features, and if you have the time, add more features.\n",
    "\n",
    "Report the RMSE when you apply your model to your training set and to your testing set. How do these numbers compare to each other, and to the RMSE from 3.2 and nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.06 seconds\n",
      "Training RMSE: 5.9629667055898095\n",
      "Testing RMSE: 4.67316824763923\n"
     ]
    }
   ],
   "source": [
    "# First trial: get indeces for respective column names (8 features, not too ambitious)\n",
    "features1 = ['CRIM','RM','CRIM*RM','RM*RM','DIS','NOX','NOX*NOX','DIS*NOX']\n",
    "index1 = []\n",
    "for i in range(len(column_names)):\n",
    "    if column_names[i] in features1:\n",
    "        index1.append(i)\n",
    "# print(index1)\n",
    "\n",
    "# standardize selected features on training set\n",
    "Xtrain6 = standardize(bdata_X_train[:, index1])\n",
    "ytrain6 = bdata_y_train\n",
    "\n",
    "# train on multivariate ols model\n",
    "inter6, coef6, iter6 = multivariate_ols(Xtrain6, ytrain6, 0.04, 100000)\n",
    "\n",
    "# compute rmse to check performance\n",
    "pred_y_6 = np.dot(Xtrain6, coef6) + inter6\n",
    "train6_rmse = compute_rmse(pred_y_6, ytrain6)\n",
    "print('Training RMSE:', train6_rmse)\n",
    "# print('\\nIntercept: ', inter6, '\\nCoefficients: ', coef6)\n",
    "\n",
    "# standardize selected features on testing set\n",
    "Xtest6 = standardize(bdata_X_test[:, index1])\n",
    "ytest6 = bdata_y_test\n",
    "\n",
    "# compute rmse to check performance on testing set with the above\n",
    "pred_y_6test = np.dot(Xtest6, coef6) + inter6\n",
    "test6_rmse = compute_rmse(pred_y_6test, ytest6)\n",
    "print('Testing RMSE:', test6_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSEs are both small for training set and testing set. Compared with other models (3.2 and NN), this model has smaller RMSE for training  testing. Testing RMSE is even smaller than Training, so it does a pretty good job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 11.40 seconds\n",
      "Training RMSE: 2.8898698268043805\n",
      "Testing RMSE: 4.096842973294588\n"
     ]
    }
   ],
   "source": [
    "# Second trial: now let's randomly select more features (80 features, quite ambitious!)\n",
    "index2 = np.random.choice(len(column_names)-1, 80, replace=False)\n",
    "# print(index2)\n",
    "\n",
    "# standardize selected features on training set\n",
    "Xtrain7 = standardize(bdata_X_train[:, index2])\n",
    "ytrain7 = bdata_y_train\n",
    "\n",
    "# train on multivariate ols model\n",
    "inter7, coef7, iter7 = multivariate_ols(Xtrain7, ytrain7, 0.04, 10000000)\n",
    "\n",
    "# compute rmse to check performance\n",
    "pred_y_7 = np.dot(Xtrain7, coef7) + inter7\n",
    "train7_rmse = compute_rmse(pred_y_7, ytrain7)\n",
    "print('Training RMSE:', train7_rmse)\n",
    "# print('\\nIntercept: ', inter7, '\\nCoefficients: ', coef7)\n",
    "\n",
    "# standardize selected features on testing set\n",
    "Xtest7 = standardize(bdata_X_test[:, index2])\n",
    "ytest7 = bdata_y_test\n",
    "\n",
    "# compute rmse to check performance on testing set with the above\n",
    "pred_y_7test = np.dot(Xtest7, coef7) + inter7\n",
    "test7_rmse = compute_rmse(pred_y_7test, ytest7)\n",
    "print('Testing RMSE:', test7_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I ambitiously and randomly throw much more features in the model, the training RMSE is getting much smaller and the testing RMSE gets slightly smaller but not as much. This is sort of overfitting in my understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ridge regularization\n",
    "Incorporate L2 (Ridge) regularization into your multivariate_ols regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
    "\n",
    "Use your regularized regression to re-fit the model from 4.2 above on your training data. Try this for several different values of lambda, and report your RMSE for each lambda separately for your training and testing data. How do these numbers compare to each other, to the RMSE from 4.2,  to the RMSE from 2.3, and to the RMSE from nearest neighbors?\n",
    "\n",
    "Go brag to your friends about how you just implemented ridge-regularized multivariate regression using gradient descent optimization, from scratch. If you still have friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_ols_ridge(xvalues, yvalues, R=0.01, MaxIterations=1000, lamb=100):\n",
    "#     start_time = time.time()\n",
    "    # Initialize variablse\n",
    "    N, m = np.shape(xvalues)\n",
    "    alpha_old = np.random.rand()\n",
    "    beta_old = np.random.rand(m)\n",
    "    alpha = np.random.rand()\n",
    "    beta = np.random.rand(m)\n",
    "    \n",
    "    # tolerance and iteration counter\n",
    "    cost_list = np.empty(1)\n",
    "    epsilon = 1e-4\n",
    "    cnt = 0\n",
    "    while(np.abs(alpha_old-alpha) > epsilon or np.max(np.abs(beta_old-beta)) > epsilon):\n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "        alpha = alpha_old - R / N * ((alpha_old +  np.dot(xvalues, beta_old) - yvalues).sum())\n",
    "        beta = beta_old - R / N * (np.dot(xvalues.T, (alpha_old +  np.dot(xvalues, beta_old) - yvalues)) \n",
    "                                   + lamb * beta_old) # ridge L2 penalization\n",
    "        \n",
    "        # print the cost to check it is descreasing\n",
    "        cost = 1.0 / (2* N) * np.linalg.norm(yvalues - alpha - np.dot(xvalues, beta))**2\n",
    "        cost_list= np.append(cost_list, cost)\n",
    "#         print(cost_list)\n",
    "        \n",
    "        # If it takes too many iterations terminate\n",
    "        cnt += 1\n",
    "        if cnt > MaxIterations:\n",
    "            print('Iterations exceeded maximum number. Stopping...')\n",
    "            break\n",
    "        \n",
    "#     print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))    \n",
    "    return alpha, beta, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 4.319830962839745 \n",
      "Iterations:  615\n",
      "Testing RMSE: 4.454881961285991\n"
     ]
    }
   ],
   "source": [
    "## Refit the second trial in 4.2, where I have 80 features\n",
    "# train on multivariate ols model with ridge regularization, lambda=50\n",
    "inter8, coef8, iter8 = multivariate_ols_ridge(Xtrain7, ytrain7, 0.04, 10000000, 50)\n",
    "\n",
    "# compute rmse to check performance\n",
    "pred_y_8 = np.dot(Xtrain7, coef8) + inter8\n",
    "train8_rmse = compute_rmse(pred_y_8, ytrain7)\n",
    "print('Training RMSE:', train8_rmse, '\\nIterations: ', iter8)\n",
    "\n",
    "# compute rmse to check performance on testing set with the above\n",
    "pred_y_8test = np.dot(Xtest7, coef8) + inter8\n",
    "test8_rmse = compute_rmse(pred_y_8test, ytest7)\n",
    "print('Testing RMSE:', test8_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 4.5712318545026385 \n",
      "Iterations:  369\n",
      "Testing RMSE: 4.591003878320222\n"
     ]
    }
   ],
   "source": [
    "## same as above, but lambda = 100 (increase penalty)\n",
    "inter8, coef8, iter8 = multivariate_ols_ridge(Xtrain7, ytrain7, 0.04, 10000000, 100)\n",
    "\n",
    "# compute rmse to check performance\n",
    "pred_y_8 = np.dot(Xtrain7, coef8) + inter8\n",
    "train8_rmse = compute_rmse(pred_y_8, ytrain7)\n",
    "print('Training RMSE:', train8_rmse, '\\nIterations: ', iter8)\n",
    "\n",
    "# compute rmse to check performance on testing set with the above\n",
    "pred_y_8test = np.dot(Xtest7, coef8) + inter8\n",
    "test8_rmse = compute_rmse(pred_y_8test, ytest7)\n",
    "print('Testing RMSE:', test8_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSEs after regularization remain at a small level, way smaller than those of 2.3 and NN. Training RMSE is about the same as testing RMSE when lambda = 50 and lambda = 100. When lambda increases, iteratoins decrease. There is definitely space to tune the parameters.\n",
    "\n",
    "Compared to 4.2, RMSE for training is getting larger and RMSE for testing is getting smaller. It means regularization is useful. Run time is way shorter too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit 2: Cross-validate lambda\n",
    "\n",
    "Use k-fold cross-validation to select the optimal value of lambda. Report the average RMSE across all training sets, and the average RMSE across all testing sets. How do these numbers compare to each other, to the RMSE from your previous efforts?  Finally, create a scatter plot that shows RMSE as a function of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice 80 features from whole dataset, standardize X array\n",
    "X_cv = standardize(np.array(bdata_df_new.iloc[:, index2]))\n",
    "y_cv = np.array(bdata_df_new.iloc[:, -1])\n",
    "\n",
    "# 25-fold cross validation for lambda\n",
    "kf = KFold(n_splits=25)\n",
    "\n",
    "train_rmse_lst = []\n",
    "test_rmse_lst = []\n",
    "for lamb in range(100, 1001, 100): # lambda from 100 to 1000, 100 at a step\n",
    "    \n",
    "    train_rmse_cv = []\n",
    "    test_rmse_cv = []\n",
    "    for train_index, test_index in kf.split(X_cv):\n",
    "        X_train, X_test = X_cv[train_index], X_cv[test_index]\n",
    "        y_train, y_test = y_cv[train_index], y_cv[test_index]\n",
    "        inter, coef, iteration = multivariate_ols_ridge(X_train, y_train, 0.04, 1000000, lamb)\n",
    "#         print(inter, coef, iteration)\n",
    "\n",
    "        pred_y_train = np.dot(X_train, coef) + inter\n",
    "        rmse_tr = compute_rmse(pred_y_train, y_train)\n",
    "        \n",
    "        pred_y_test = np.dot(X_test, coef) + inter\n",
    "        rmse_te = compute_rmse(pred_y_test, y_test)\n",
    "        \n",
    "        train_rmse_cv.append(rmse_tr)\n",
    "        test_rmse_cv.append(rmse_te)\n",
    "    \n",
    "    train_rmse_lst.append(np.mean(train_rmse_cv))\n",
    "    test_rmse_lst.append(np.mean(test_rmse_cv))\n",
    "\n",
    "# print(train_rmse_lst, test_rmse_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training RMSE for each lambda:  [4.329845002609694, 4.552379527662397, 4.68409578725884, 4.785125420540549, 4.870857759405928, 4.947201108428253, 5.01703619649792, 5.081957315290999, 5.142934258326165, 5.200604637731724] \n",
      "Average testing RMSE for each lambda:  [4.656478128101492, 4.754913911757529, 4.817795333739116, 4.870934816486276, 4.919900281665656, 4.966200502705851, 5.01051901890798, 5.053154806262375, 5.0943147500978005, 5.13412647424658]\n"
     ]
    }
   ],
   "source": [
    "print('Average training RMSE for each lambda: ', train_rmse_lst, '\\nAverage testing RMSE for each lambda: ', test_rmse_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(range(100,1001,100), train_rmse_lst, c='b', label='Train average RMSE')\n",
    "plt.scatter(range(100,1001,100), test_rmse_lst, c='r', label='Test average RMSE')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('lamda from 50 to 500, step=100')\n",
    "plt.ylabel('Average RMSE for each lambda')\n",
    "plt.title('K-fold Cross-validation (k=25) for Lambda between 100 and 1000 (step at 100)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When lambda is somewhere above 700, the average Test RMSE will be lower than the average Train RMSE, meaning the overfitting can be regulated more effectively. However, since we have 80 features in the model, the RMSE is lifted up when we increase the penalties. The RMSEs are higher than RMSEs in 4.3 (train rmse=2.88, test rmse=4.09)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extra Credit 3: AdaGrad\n",
    "\n",
    "AdaGrad is a method to implement gradient descent with different learning rates for each feature. Adaptive algorithms like this one are being extensively used especially in neural network training. Implement AdaGrad on 2.3 but now use CRIM, RM and DIS as independent variables. Standardize these variables before inputting them to the gradient descent algorithm. Tune the algorithm until you estimate the regression coefficients within a tolerance of 1e-1. Use mini-batch gradient descent in this implementation. In summary for each parameter (in our case one intercept and three slopes) the update step of the gradient (in this example $\\beta_j$) at iteration $k$ of the GD algorithm becomes:\n",
    "\n",
    "$$\\beta_j=\\beta_j -\\frac{R}{\\sqrt{G^{(k)}_j}}\\frac{\\partial J(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$$ where\n",
    "$G^{(k)}_j=\\sum_{i=1}^{k} (\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j})^2$ and $R$ is your learning rate. The notation $\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$ corresponds to the value of the gradient at iteration $(i)$. Essentially we are \"storing\" information about previous iteration gradients. Doing that we effectively decrease the learning rate slower when a feature $x_i$ is sparse (i.e. has many zero values which would lead to zero gradients). Although this method is not necessary for our regression problem, it is good to be familiar with these methods as they are widely used in neural network training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "APIs_geospatial",
   "language": "python",
   "name": "apis_geospatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
